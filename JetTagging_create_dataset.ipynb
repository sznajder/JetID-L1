{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK0wr2onBn4d"
   },
   "source": [
    "# Create Dataset for Jet Tagging @ L1 studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.8.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Number of available GPUs : 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow {tf.__version__}\")\n",
    "print(tf.config.list_physical_devices())\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    print(f\"Number of available GPUs : {len(gpus)}\")\n",
    "    tf.config.set_visible_devices(gpus[0],\"GPU\")\n",
    "    tf.config.experimental.set_memory_growth(gpus[0],True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU !!!\")    \n",
    "\n",
    "# To disable GPU use\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Standization Methods for HLS Jet Data \n",
    "### From Patrick :   \n",
    "### https://github.com/bb511/know_dist/blob/main/preprocessing/standardisation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation methods for the jet data.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "def apply_standardisation(choice: str, x_data: np.ndarray, feat_range: tuple = (0, 1)):\n",
    "    \"\"\"Choose the type of normalisation to apply to the data.\n",
    "\n",
    "    Args:\n",
    "        choice: The choice of the user with repsect to the type of norm to apply.\n",
    "        x_data: Array containing the data to normalise.\n",
    "        feat_range: The range that each feature will be normalised to, e.g., in minmax.\n",
    "\n",
    "    Returns:\n",
    "        Normalised x_data numpy array.\n",
    "    \"\"\"\n",
    "    if choice == \"nonorm\":\n",
    "        print(\"Skipping normalisation...\")\n",
    "        return x_data\n",
    "\n",
    "    print(f\"Applying {choice} normalisation...\")\n",
    "    switcher = {\n",
    "        \"minmax\": lambda: minmax(x_data, feat_range),\n",
    "        \"robust\": lambda: robust(x_data),\n",
    "        \"standard\": lambda: standard(x_data),\n",
    "    }\n",
    "\n",
    "    x_data = switcher.get(choice, lambda: None)()\n",
    "\n",
    "    if x_data is None:\n",
    "        raise NameError(\n",
    "            \"Type of normalisation does not exist! Please choose from \"\n",
    "            f\"the following list: {list(switcher.keys())}\"\n",
    "        )\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def minmax(x: np.ndarray, feature_range: tuple = (0, 1)) -> np.ndarray:\n",
    "    \"\"\"Applies minmax normalisation to the data, i.e., every feature of every sample\n",
    "    is divided by the maximum for that respective feature.\n",
    "\n",
    "    Args:\n",
    "        x: Data array.\n",
    "        feature_range: Minimum and maximum of features after the normalisation.\n",
    "    \"\"\"\n",
    "    min_feats = x.min(axis=0).min(axis=0)\n",
    "    max_feats = x.max(axis=0).max(axis=0)\n",
    "    x_norm = (x - min_feats) / (max_feats - min_feats)\n",
    "    x_norm = x_norm * (feature_range[1] - feature_range[0]) + feature_range[0]\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "def robust(x: np.ndarray, percentiles: list = [95, 5]) -> np.ndarray:\n",
    "    \"\"\"Applies robust normalisation to the data, i.e., the median of every feature is\n",
    "    subtracted from every respective sample belonging to that feature and then each\n",
    "    feature is scaled with respect to the respective inter-quantile range between\n",
    "    the 1st and 3rd quantiles.\n",
    "\n",
    "    Args:\n",
    "        x: Data array.\n",
    "        percentiles: Between which percentiles to normalise. The default is from the\n",
    "            google interaction network paper. The sklearn standard is [75, 25].\n",
    "    \"\"\"\n",
    "    x_median = []\n",
    "    interquantile_range = []\n",
    "\n",
    "    for feature_idx in range(x.shape[-1]):\n",
    "        x_feature = x[:, :, feature_idx].flatten()\n",
    "        x_median.append(np.nanmedian(x_feature, axis=0))\n",
    "        quantile_high, quantile_low = np.nanpercentile(x_feature, percentiles)\n",
    "        interquantile_range.append(quantile_high - quantile_low)\n",
    "\n",
    "    x_norm = (x - x_median) / interquantile_range\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "def standard(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Applies standard normalisation to the data, i.e., the mean of each feature is\n",
    "    subtracted from every sample belonging to the respective feature and then divided\n",
    "    by the corresponding standard deviation.\n",
    "    \"\"\"\n",
    "    x_mean = []\n",
    "    x_std = []\n",
    "\n",
    "    for feature_idx in range(x.shape[-1]):\n",
    "        x_feature = x[:, :, feature_idx].flatten()\n",
    "        x_mean.append(x_feature.mean(axis=0))\n",
    "        x_std.append(x_feature.std(axis=0))\n",
    "\n",
    "    x_norm = (x - x_mean) / x_std\n",
    "\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxGwUjLmBn4f"
   },
   "source": [
    "# Load HLS4ML dataset \n",
    "\n",
    "Here, we load the numpy arrays containing the 4D tensors of \"jet-images\" (see https://arxiv.org/pdf/1511.05190.pdf)\n",
    "\n",
    "https://github.com/pierinim/tutorials/blob/master/GGI_Jan2021/Lecture1/Notebook1_ExploreDataset.ipynb\n",
    "\n",
    " * `jetImage` contains the image representation of the jets (more later)\n",
    " * `jetImageECAL` and `jetImageHCAL` are the ECAL- and HCAL-only equivalent images. We will not use them (but you are more than welcome to play with it)\n",
    " * `jetConstituentList` is the list of particles cointained in the jet. For each particle, a list of relevant quantities is stored\n",
    " * `particleFeatureNames` is the list of the names corresponding to the quantities contained in `jetConstituentList`; `jets` is the dataset we consider for the moment\n",
    " * `jetFeatureNames` is the list of the names corresponding to the quantities contained in `jets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1616956528521,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "TcXokNduBn4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending jetImage_6_150p_40000_50000.h5\n",
      "Keys in H5PY files =  ['jetConstituentList', 'jetFeatureNames', 'jetImage', 'jetImageECAL', 'jetImageHCAL', 'jets', 'particleFeatureNames']\n",
      " \n",
      "Jets Features =  [b'j_ptfrac' b'j_pt' b'j_eta' b'j_mass' b'j_tau1_b1' b'j_tau2_b1'\n",
      " b'j_tau3_b1' b'j_tau1_b2' b'j_tau2_b2' b'j_tau3_b2' b'j_tau32_b1'\n",
      " b'j_tau32_b2' b'j_zlogz' b'j_c1_b0' b'j_c1_b1' b'j_c1_b2' b'j_c2_b1'\n",
      " b'j_c2_b2' b'j_d2_b1' b'j_d2_b2' b'j_d2_a1_b1' b'j_d2_a1_b2' b'j_m2_b1'\n",
      " b'j_m2_b2' b'j_n2_b1' b'j_n2_b2' b'j_tau1_b1_mmdt' b'j_tau2_b1_mmdt'\n",
      " b'j_tau3_b1_mmdt' b'j_tau1_b2_mmdt' b'j_tau2_b2_mmdt' b'j_tau3_b2_mmdt'\n",
      " b'j_tau32_b1_mmdt' b'j_tau32_b2_mmdt' b'j_c1_b0_mmdt' b'j_c1_b1_mmdt'\n",
      " b'j_c1_b2_mmdt' b'j_c2_b1_mmdt' b'j_c2_b2_mmdt' b'j_d2_b1_mmdt'\n",
      " b'j_d2_b2_mmdt' b'j_d2_a1_b1_mmdt' b'j_d2_a1_b2_mmdt' b'j_m2_b1_mmdt'\n",
      " b'j_m2_b2_mmdt' b'j_n2_b1_mmdt' b'j_n2_b2_mmdt' b'j_mass_trim'\n",
      " b'j_mass_mmdt' b'j_mass_prun' b'j_mass_sdb2' b'j_mass_sdm1'\n",
      " b'j_multiplicity' b'j_g' b'j_q' b'j_w' b'j_z' b'j_t' b'j_undef']\n",
      " \n",
      "Jet Constituents Features =  [b'j1_px' b'j1_py' b'j1_pz' b'j1_e' b'j1_erel' b'j1_pt' b'j1_ptrel'\n",
      " b'j1_eta' b'j1_etarel' b'j1_etarot' b'j1_phi' b'j1_phirel' b'j1_phirot'\n",
      " b'j1_deltaR' b'j1_costheta' b'j1_costhetarel' b'j1_pdgid']\n",
      " \n",
      "Jet Images =  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "Jet Image Shape =  (10000, 100, 100)\n",
      " \n",
      "Jet Pt= [1010.34552002 1110.35253906  987.69482422 ... 1000.4309082   997.0201416\n",
      "  989.32739258]\n",
      " \n",
      "Appending jetImage_3_150p_10000_20000.h5\n",
      "Appending jetImage_2_150p_10000_20000.h5\n",
      "Appending jetImage_7_150p_40000_50000.h5\n",
      "Appending jetImage_8_150p_20000_30000.h5\n",
      "Appending jetImage_1_150p_10000_20000.h5\n",
      "Appending jetImage_4_150p_40000_50000.h5\n",
      "Appending jetImage_4_150p_0_10000.h5\n",
      "Appending jetImage_9_150p_20000_30000.h5\n",
      "Appending jetImage_5_150p_40000_50000.h5\n",
      "Appending jetImage_0_150p_10000_20000.h5\n",
      "Appending jetImage_6_150p_10000_20000.h5\n",
      "Appending jetImage_3_150p_0_10000.h5\n",
      "Appending jetImage_3_150p_40000_50000.h5\n",
      "Appending jetImage_2_150p_40000_50000.h5\n",
      "Appending jetImage_7_150p_10000_20000.h5\n",
      "Appending jetImage_8_150p_70000_80000.h5\n",
      "Appending jetImage_1_150p_40000_50000.h5\n",
      "Appending jetImage_4_150p_10000_20000.h5\n",
      "Appending jetImage_8_150p_80000_90000.h5\n",
      "Appending jetImage_5_150p_10000_20000.h5\n",
      "Appending jetImage_0_150p_40000_50000.h5\n",
      "Appending jetImage_9_150p_70000_80000.h5\n",
      "Appending jetImage_9_150p_80000_90000.h5\n",
      "Appending jetImage_2_150p_0_10000.h5\n",
      "Appending jetImage_4_150p_80000_90000.h5\n",
      "Appending jetImage_1_150p_20000_30000.h5\n",
      "Appending jetImage_8_150p_10000_20000.h5\n",
      "Appending jetImage_4_150p_70000_80000.h5\n",
      "Appending jetImage_0_150p_20000_30000.h5\n",
      "Appending jetImage_5_150p_80000_90000.h5\n",
      "Appending jetImage_5_150p_70000_80000.h5\n",
      "Appending jetImage_9_150p_10000_20000.h5\n",
      "Appending jetImage_6_150p_70000_80000.h5\n",
      "Appending jetImage_3_150p_20000_30000.h5\n",
      "Appending jetImage_6_150p_80000_90000.h5\n",
      "Appending jetImage_7_150p_70000_80000.h5\n",
      "Appending jetImage_7_150p_80000_90000.h5\n",
      "Appending jetImage_2_150p_20000_30000.h5\n",
      "Appending jetImage_1_150p_70000_80000.h5\n",
      "Appending jetImage_8_150p_40000_50000.h5\n",
      "Appending jetImage_4_150p_20000_30000.h5\n",
      "Appending jetImage_1_150p_80000_90000.h5\n",
      "Appending jetImage_0_150p_70000_80000.h5\n",
      "Appending jetImage_0_150p_80000_90000.h5\n",
      "Appending jetImage_5_150p_20000_30000.h5\n",
      "Appending jetImage_3_150p_80000_90000.h5\n",
      "Appending jetImage_6_150p_20000_30000.h5\n",
      "Appending jetImage_3_150p_70000_80000.h5\n",
      "Appending jetImage_7_150p_20000_30000.h5\n",
      "Appending jetImage_2_150p_80000_90000.h5\n",
      "Appending jetImage_5_150p_0_10000.h5\n",
      "Appending jetImage_2_150p_70000_80000.h5\n",
      "Appending jetImage_5_150p_30000_40000.h5\n",
      "Appending jetImage_6_150p_50000_60000.h5\n",
      "Appending jetImage_0_150p_0_10000.h5\n",
      "Appending jetImage_7_150p_50000_60000.h5\n",
      "Appending jetImage_4_150p_30000_40000.h5\n",
      "Appending jetImage_4_150p_50000_60000.h5\n",
      "Appending jetImage_7_150p_30000_40000.h5\n",
      "Appending jetImage_6_150p_30000_40000.h5\n",
      "Appending jetImage_5_150p_50000_60000.h5\n",
      "Appending jetImage_0_150p_30000_40000.h5\n",
      "Appending jetImage_1_150p_30000_40000.h5\n",
      "Appending jetImage_2_150p_50000_60000.h5\n",
      "Appending jetImage_8_150p_0_10000.h5\n",
      "Appending jetImage_8_150p_60000_70000.h5\n",
      "Appending jetImage_2_150p_30000_40000.h5\n",
      "Appending jetImage_7_150p_0_10000.h5\n",
      "Appending jetImage_1_150p_50000_60000.h5\n",
      "Appending jetImage_9_150p_60000_70000.h5\n",
      "Appending jetImage_0_150p_50000_60000.h5\n",
      "Appending jetImage_3_150p_30000_40000.h5\n",
      "Appending jetImage_4_150p_60000_70000.h5\n",
      "Appending jetImage_9_150p_0_10000.h5\n",
      "Appending jetImage_5_150p_60000_70000.h5\n",
      "Appending jetImage_6_150p_60000_70000.h5\n",
      "Appending jetImage_6_150p_0_10000.h5\n",
      "Appending jetImage_7_150p_60000_70000.h5\n",
      "Appending jetImage_1_150p_60000_70000.h5\n",
      "Appending jetImage_8_150p_50000_60000.h5\n",
      "Appending jetImage_0_150p_60000_70000.h5\n",
      "Appending jetImage_9_150p_50000_60000.h5\n",
      "Appending jetImage_1_150p_0_10000.h5\n",
      "Appending jetImage_9_150p_30000_40000.h5\n",
      "Appending jetImage_3_150p_60000_70000.h5\n",
      "Appending jetImage_8_150p_30000_40000.h5\n",
      "Appending jetImage_2_150p_60000_70000.h5\n",
      "Target shape = (880000, 5)\n",
      "Jet Constituents shape = (880000, 150, 3)\n",
      "Jet Pt shape = (880000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# for pT, eta_rel, phi_rel\n",
    "#   myJetConstituentList = np.array(f.get(\"jetConstituentList\")[:,:,[5,8,11]])\n",
    "# for px, py, pz\n",
    "#   myJetConstituentList = np.array(f.get(\"jetConstituentList\")[:,:,[0,1,2]])\n",
    "#   myJetConstituentList = np.array(f.get(\"jetConstituentList\"))\n",
    "#\n",
    "# Jet Constituents Features =  [0='j1_px', 1='j1_py', 2='j1_pz', 3='j1_e', 4='j1_erel', 5='j1_pt', 6='j1_ptrel',\n",
    "#                         7='j1_eta', 8='j1_etarel', 9='j1_etarot', 10='j1_phi', 11='j1_phirel', 12='j1_phirot',\n",
    "#                         13='j1_deltaR', 14='j1_costheta', 15='j1_costhetarel', 16='j1_pdgid']\n",
    "\n",
    "#Data PATH\n",
    "TRAIN_PATH = '/Users/sznajder/WorkM1/workdir/data/hls4ml_LHCjet_150p/'\n",
    "\n",
    "\n",
    "\n",
    "first=True\n",
    "for file in os.listdir(TRAIN_PATH):\n",
    "  print(\"Appending %s\" %file)\n",
    "\n",
    "  with h5py.File(TRAIN_PATH+file, 'r') as data:\n",
    "    if first : \n",
    "        first=False\n",
    "        jetConstituent = data['jetConstituentList'][:,:,[5,8,11]]\n",
    "        target = data['jets'][:,-6:-1]\n",
    "        jet_pt = data['jets'][:,1]\n",
    "        print(\"Keys in H5PY files = \",list( data.keys() ))\n",
    "        print(\" \")\n",
    "        featurenames = data.get('jetFeatureNames')\n",
    "        print(\"Jets Features = \",featurenames[:])\n",
    "        print(\" \")\n",
    "        featurenames = data.get('particleFeatureNames')\n",
    "        print(\"Jet Constituents Features = \",featurenames[:])\n",
    "        print(\" \")\n",
    "        images = data.get('jetImage')\n",
    "        print(\"Jet Images = \",images[:])        \n",
    "        print(\"Jet Image Shape = \",images.shape)   \n",
    "        print(\" \")\n",
    "        print(\"Jet Pt=\", jet_pt)\n",
    "        print(\" \")\n",
    "    else:\n",
    "         # Read (Pt,Etarel,Phirel)\n",
    "        jetConstituent = np.concatenate( [ jetConstituent, data['jetConstituentList'][:,:,[5,8,11]] ] , axis=0 )\n",
    "        target   = np.concatenate( [ target, data['jets'][:,-6:-1] ] , axis=0 )\n",
    "        jet_pt   = np.concatenate( [ jet_pt, data['jets'][:,1]     ] , axis=0 )\n",
    "\n",
    "print(\"Target shape =\", target.shape)\n",
    "print(\"Jet Constituents shape =\", jetConstituent.shape)\n",
    "print(\"Jet Pt shape =\", jet_pt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Image dataset labels ( ONE HOT ENCODING )\n",
    "\n",
    "Jets can be converted to images considering the (&eta;, &phi;) plane, centered along the axis direction and binned.\n",
    "In our case, we consider a square of 1.6x1.6 in size (because the jet size is R=0.8) binned in 100x100 equal-size 'cells'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth is incorporated in the `['g', 'q', 'w', 'z', 't']` vector of boolean, taking the form\n",
    "- `[1, 0, 0, 0, 0]` for gluons\n",
    "- `[0, 1, 0, 0, 0]` for quarks\n",
    "- `[0, 0, 1, 0, 0]` for Ws\n",
    "- `[0, 0, 0, 1, 0]` for Zs\n",
    "- `[0, 0, 0, 0, 1]` for tops\n",
    "\n",
    "This is what is called 'one-hot' encoding of a descrete label (typical of ground truth for classification problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Pt<2GeV constituents and Shuffles constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3265,
     "status": "ok",
     "timestamp": 1616956528521,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "YAJhuubTBn4k",
    "outputId": "8dc7af15-bbc1-4421-861a-72dee1979a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jets = 880000\n",
      "Number of constituents = 8\n",
      "Number of features = 3\n",
      "Before Shuffling --->> jetConstituent[0,0:4,0] =  [118.65741  113.409935 113.007545 104.92594 ]\n",
      "After Shuffling  --->> jetConstituent[0,0:4,0] =  [ 45.74336  118.65741  104.92594  113.409935]\n",
      "Using Feature Normalization  --->> none \n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# Convert target format from one-hot encoding to single neuron\n",
    "#target = np.argmax(target, axis=1)\n",
    "\n",
    "# The dataset is N_jets x N_constituents x N_features\n",
    "njet     = jetConstituent.shape[0]\n",
    "nconstit = jetConstituent.shape[1]\n",
    "nfeat    = jetConstituent.shape[2]\n",
    "\n",
    "\n",
    "# Filter out constituents with Pt<2GeV\n",
    "Ptmin =2. \n",
    "constituents = np.zeros((njet, nconstit, nfeat) , dtype=np.float32) \n",
    "ij=0\n",
    "max_constit=0\n",
    "for j in range(njet):\n",
    "    ic=0\n",
    "    for c in range(nconstit):\n",
    "        if ( jetConstituent[j,c,0] < Ptmin ):\n",
    "            continue\n",
    "        constituents[ij,ic,:] = jetConstituent[j,c,:] \n",
    "        ic+=1\n",
    "    if (ic > 0):\n",
    "        if ic > max_constit: max_constit=ic\n",
    "        target[ij,:]=target[j,:] # assosicate the correct target a given graph \n",
    "        jet_pt[ij]=jet_pt[j]\n",
    "        ij+=1\n",
    "\n",
    "\n",
    "# Resizes the jets constituents and target arrays        \n",
    "jetConstituent = constituents[0:ij,0:max_constit,:]\n",
    "target = target[0:ij,:]\n",
    "jet_pt = jet_pt[0:ij]\n",
    "\n",
    "# Restric the number of constituents to a maximum of NMAX\n",
    "nmax = 8\n",
    "jetConstituent = jetConstituent[:,0:nmax,:]\n",
    "\n",
    "# The dataset is N_jets x N_constituents x N_features\n",
    "njet     = jetConstituent.shape[0]\n",
    "nconstit = jetConstituent.shape[1]\n",
    "nfeat    = jetConstituent.shape[2]\n",
    "\n",
    "\n",
    "print('Number of jets =',njet)\n",
    "print('Number of constituents =',nconstit)\n",
    "print('Number of features =',nfeat)\n",
    "\n",
    "\n",
    "# Shuffles jet constituents\n",
    "print(\"Before Shuffling --->> jetConstituent[0,0:4,0] = \",jetConstituent[0,0:4,0])\n",
    "for i in range(jetConstituent.shape[0]):\n",
    "  jetConstituent[i] = jetConstituent[i, np.random.permutation(nconstit), :]\n",
    "print(\"After Shuffling  --->> jetConstituent[0,0:4,0] = \",jetConstituent[0,0:4,0])\n",
    "\n",
    "\n",
    "# Normalize data features using Patrick's code\n",
    "norm = \"none\"\n",
    "#norm = \"robust\"\n",
    "#norm = \"standard\"\n",
    "#norm = \"minmax\"\n",
    "#norm = \"rescale\"\n",
    "\n",
    "print(\"Using Feature Normalization  --->> {} \".format(norm))\n",
    "\n",
    "if ( norm==\"rescale\" ):\n",
    "# Rescale constituents features by \n",
    "  for j in range(njet):\n",
    "#    jetConstituent[j,:,0]=jetConstituent[j,:,0]/jet_pt[j]\n",
    "    jetConstituent[j,:,0]=jetConstituent[j,:,0]/100.\n",
    "    jetConstituent[j,:,1]=jetConstituent[j,:,1]/10.\n",
    "    jetConstituent[j,:,2]=jetConstituent[j,:,2]/6.5  \n",
    "  print(\"After Normalization {} --->> jetConstituent[0,0:4,0] = \".format(norm),jetConstituent[0,0:4,0])\n",
    "elif ( norm!=\"none\" ):\n",
    "  jetConstituent = apply_standardisation( norm, jetConstituent )\n",
    "  print(\"After Normalization {} --->> jetConstituent[0,0:4,0] = \".format(norm),jetConstituent[0,0:4,0])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into testing and training dataset\n",
    "\n",
    "We will split the data into two parts (one for training+validation and one for testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(589600, 8, 3) (290400, 8, 3) (589600, 5) (290400, 5)\n",
      "number of G jets for training/validation: 118789\n",
      "number of Q jets for training/validation: 114175\n",
      "number of W jets for training/validation: 118845\n",
      "number of Z jets for training/validation: 118615\n",
      "number of T jets for training/validation: 119176\n",
      "number of G jets for testing: 58463\n",
      "number of Q jets for testing: 56504\n",
      "number of W jets for testing: 58327\n",
      "number of Z jets for testing: 58337\n",
      "number of T jets for testing: 58769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = jetConstituent\n",
    "Y = target\n",
    "del jetConstituent , target\n",
    "\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "print(X_train_val.shape, X_test.shape, Y_train_val.shape, Y_test.shape)\n",
    "\n",
    "print('number of G jets for training/validation: %i'%np.sum( np.argmax(Y_train_val, axis=1)==0 ))\n",
    "print('number of Q jets for training/validation: %i'%np.sum( np.argmax(Y_train_val, axis=1)==1 ))\n",
    "print('number of W jets for training/validation: %i'%np.sum( np.argmax(Y_train_val, axis=1)==2 ))\n",
    "print('number of Z jets for training/validation: %i'%np.sum( np.argmax(Y_train_val, axis=1)==3 ))\n",
    "print('number of T jets for training/validation: %i'%np.sum( np.argmax(Y_train_val, axis=1)==4 ))\n",
    "\n",
    "\n",
    "print('number of G jets for testing: %i'%np.sum( np.argmax(Y_test, axis=1)==0 ))\n",
    "print('number of Q jets for testing: %i'%np.sum( np.argmax(Y_test, axis=1)==1 ))\n",
    "print('number of W jets for testing: %i'%np.sum( np.argmax(Y_test, axis=1)==2 ))\n",
    "print('number of Z jets for testing: %i'%np.sum( np.argmax(Y_test, axis=1)==3 ))\n",
    "print('number of T jets for testing: %i'%np.sum( np.argmax(Y_test, axis=1)==4 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves the DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\"../../data/X_train_val_nconst_{}_norm_{}\".format(nconstit,norm), X_train_val)\n",
    "np.save(\"../../data/X_test_nconst_{}_norm_{}\".format(nconstit,norm)     , X_test)\n",
    "np.save(\"../../data/Y_train_val_nconst_{}_norm_{}\".format(nconstit,norm), Y_train_val)\n",
    "np.save(\"../../data/Y_test_nconst_{}_norm_{}\".format(nconstit,norm)     , Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JetTagging_MLP_TFKeras.ipynb",
   "provenance": [
    {
     "file_id": "1_LtW5Af1ruCzp6IH18NNj2K3Vbgqto-F",
     "timestamp": 1613800914899
    },
    {
     "file_id": "https://github.com/thongonary/machine_learning_vbscan/blob/master/5-conv2d.ipynb",
     "timestamp": 1551264063701
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
