{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ec338e-f232-4e3f-88fb-d75bdcdc409f",
   "metadata": {},
   "source": [
    "# Notebook for KFOLD training based on Walkie's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9849ca14-7e27-4830-a4a8-14b887ae8a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow {tf.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    print(f\"Number of available GPUs : {len(gpus)}\")\n",
    "    tf.config.set_visible_devices(gpus[0],\"GPU\")\n",
    "    tf.config.experimental.set_memory_growth(gpus[0],True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU !!!\")    \n",
    "    \n",
    "# To disable GPU use\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "'''\n",
    "    \n",
    "# with tf.device('CPU: 0'):\n",
    "# with tf.device('GPU: 0'): \n",
    "\n",
    "#tf.device('CPU: 0')\n",
    "\n",
    "\n",
    "# Define model hyperparameters \n",
    "seed = 175   # random seed\n",
    "kfolds = 5   # number o folds\n",
    "nfeat=3      # number of constituents features\n",
    "batch=256    # batch size\n",
    "nepochs=300   # number of epochs\n",
    "#lr=0.0005    # learning rate\n",
    "lr=0.0002    # learning rate\n",
    "patience=10  # patience \n",
    "\n",
    "# Set numer of constituents and bitwidth\n",
    "nconstit=16   # number of constituents\n",
    "nbits=8      # QKeras bitwidth\n",
    "\n",
    "# Define architecture name\n",
    "arch = \"QGCN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72ca86-8b55-41cb-9a77-ef4d6b56b43a",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a754499-767d-46e3-ab69-de0be8bcc79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainign with max # of contituents =  16\n",
      "Number of node features =  3\n",
      "Quantization with nbits= 8\n",
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 05:49:53.777342: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-23 05:49:53.777668: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainign with max # of contituents =  16\n",
      "Number of node features =  3\n",
      "Quantization with nbits= 8\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inp (InputLayer)               [(None, 16, 3)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1D_2 (QConv1D)             (None, 16, 40)       160         ['inp[0][0]']                    \n",
      "                                                                                                  \n",
      " activ_conv_1 (QActivation)     (None, 16, 40)       0           ['conv1D_2[0][0]']               \n",
      "                                                                                                  \n",
      " avgpool_1 (GlobalAveragePoolin  (None, 40)          0           ['activ_conv_1[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1D_1 (QConv1D)             (None, 16, 40)       160         ['inp[0][0]']                    \n",
      "                                                                                                  \n",
      " reshap (Reshape)               (None, 1, 40)        0           ['avgpool_1[0][0]']              \n",
      "                                                                                                  \n",
      " activ_conv_2 (QActivation)     (None, 16, 40)       0           ['conv1D_1[0][0]']               \n",
      "                                                                                                  \n",
      " upsampl (UpSampling1D)         (None, 16, 40)       0           ['reshap[0][0]']                 \n",
      "                                                                                                  \n",
      " add1 (Add)                     (None, 16, 40)       0           ['activ_conv_2[0][0]',           \n",
      "                                                                  'upsampl[0][0]']                \n",
      "                                                                                                  \n",
      " activ_aggregation (QActivation  (None, 16, 40)      0           ['add1[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " linear_activ (QActivation)     (None, 16, 40)       0           ['activ_aggregation[0][0]']      \n",
      "                                                                                                  \n",
      " avgpool_2 (AveragePooling1D)   (None, 2, 40)        0           ['linear_activ[0][0]']           \n",
      "                                                                                                  \n",
      " Flatten (Flatten)              (None, 80)           0           ['avgpool_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (QDense)               (None, 50)           4050        ['Flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activ_dense_1 (QActivation)    (None, 50)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " denseOut (QDense)              (None, 5)            255         ['activ_dense_1[0][0]']          \n",
      "                                                                                                  \n",
      " softmx (Activation)            (None, 5)            0           ['denseOut[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,625\n",
      "Trainable params: 4,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    GlobalAveragePooling1D,\n",
    "    AveragePooling1D,\n",
    "    Reshape,\n",
    "    UpSampling1D,\n",
    "    Add,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from qkeras import QActivation, QDense, QConv1D, QConv2D, quantized_bits\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# Quantized bits\n",
    "\n",
    "#qbits = quantized_bits(nbits,integ,alpha=1.0)\n",
    "#qact = 'quantized_relu('+str(nbits)+',0)'\n",
    "\n",
    "# Set QKeras quantizer and activation \n",
    "if nbits == 1:\n",
    "    qbits = 'binary(alpha=1)'\n",
    "elif nbits == 2:\n",
    "    qbits = 'ternary(alpha=1)'\n",
    "else:\n",
    "    qbits = 'quantized_bits({},0,alpha=1)'.format(nbits)\n",
    "\n",
    "qact = 'quantized_relu({},0)'.format(nbits)\n",
    "\n",
    "# Set QKeras linear activation quantization for CONV1D layers to avoid AveragePooling overflow\n",
    "conv_qbits = 'quantized_bits(15,7)'\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# Load the model definition\n",
    "if (arch==\"QMLP\"):\n",
    "    execfile('mlp.py',globals(),locals())\n",
    "elif (arch==\"QGCN\"):\n",
    "    execfile('gcn.py',globals(),locals())\n",
    "else:\n",
    "    print(\"UNKNOWN ARCH !!! \",arch)\n",
    "    stop\n",
    "      \n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# Print\n",
    "print(\"Trainign with max # of contituents = \", nconstit)\n",
    "print(\"Number of node features = \", nfeat)\n",
    "print(\"Quantization with nbits=\",nbits)\n",
    "\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Define the optimizer ( minimization algorithm )\n",
    "optim = Adam(learning_rate=lr)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Save model weights before training to re-initialize weights in each folding\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed719dc8-1b92-4a04-8c82-cfb7eabdb0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kfold num: 0\n",
      "\n",
      "----------------\n",
      "Data loading complete:\n",
      "File name: None\n",
      "Training data size: 480,840\n",
      "Test data size: 120,210\n",
      "Number of constituents: 16\n",
      "Number of features: 3\n",
      "----------------\n",
      "\n",
      "(480840, 16, 3)\n",
      "(480840, 16, 3) (120210, 16, 3) (480840, 5) (120210, 5)\n",
      "number of G jets for training/validation: 96168/24042\n",
      "number of Q jets for training/validation: 96168/24042\n",
      "number of W jets for training/validation: 96168/24042\n",
      "number of Z jets for training/validation: 96168/24042\n",
      "number of T jets for training/validation: 96168/24042\n",
      "number of G jets for testing: 24042\n",
      "number of Q jets for testing: 24042\n",
      "number of W jets for testing: 24042\n",
      "number of Z jets for testing: 24042\n",
      "number of T jets for testing: 24042\n",
      "output dir:  model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 05:49:56.759103: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1879 [..............................] - ETA: 25:04 - loss: 1.6510 - categorical_accuracy: 0.1289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 05:49:57.406671: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878/1879 [============================>.] - ETA: 0s - loss: 1.3477 - categorical_accuracy: 0.4500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 05:50:11.786895: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.52479, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 1.3477 - categorical_accuracy: 0.4500 - val_loss: 1.2033 - val_categorical_accuracy: 0.5248 - lr: 2.0000e-04\n",
      "Epoch 2/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 1.1566 - categorical_accuracy: 0.5417\n",
      "Epoch 2: val_categorical_accuracy improved from 0.52479 to 0.55314, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 19s 10ms/step - loss: 1.1566 - categorical_accuracy: 0.5417 - val_loss: 1.1275 - val_categorical_accuracy: 0.5531 - lr: 2.0000e-04\n",
      "Epoch 3/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 1.1131 - categorical_accuracy: 0.5603\n",
      "Epoch 3: val_categorical_accuracy improved from 0.55314 to 0.56591, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 18s 10ms/step - loss: 1.1131 - categorical_accuracy: 0.5603 - val_loss: 1.1020 - val_categorical_accuracy: 0.5659 - lr: 2.0000e-04\n",
      "Epoch 4/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 1.0921 - categorical_accuracy: 0.5706\n",
      "Epoch 4: val_categorical_accuracy improved from 0.56591 to 0.57284, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 1.0921 - categorical_accuracy: 0.5706 - val_loss: 1.0849 - val_categorical_accuracy: 0.5728 - lr: 2.0000e-04\n",
      "Epoch 5/300\n",
      "1878/1879 [============================>.] - ETA: 0s - loss: 1.0774 - categorical_accuracy: 0.5782\n",
      "Epoch 5: val_categorical_accuracy improved from 0.57284 to 0.58028, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 19s 10ms/step - loss: 1.0774 - categorical_accuracy: 0.5782 - val_loss: 1.0720 - val_categorical_accuracy: 0.5803 - lr: 2.0000e-04\n",
      "Epoch 6/300\n",
      "1873/1879 [============================>.] - ETA: 0s - loss: 1.0661 - categorical_accuracy: 0.5834\n",
      "Epoch 6: val_categorical_accuracy improved from 0.58028 to 0.58546, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 1.0660 - categorical_accuracy: 0.5834 - val_loss: 1.0623 - val_categorical_accuracy: 0.5855 - lr: 2.0000e-04\n",
      "Epoch 7/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 1.0559 - categorical_accuracy: 0.5877\n",
      "Epoch 7: val_categorical_accuracy improved from 0.58546 to 0.58900, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 1.0560 - categorical_accuracy: 0.5877 - val_loss: 1.0523 - val_categorical_accuracy: 0.5890 - lr: 2.0000e-04\n",
      "Epoch 8/300\n",
      "1873/1879 [============================>.] - ETA: 0s - loss: 1.0470 - categorical_accuracy: 0.5916\n",
      "Epoch 8: val_categorical_accuracy improved from 0.58900 to 0.59135, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 1.0469 - categorical_accuracy: 0.5916 - val_loss: 1.0443 - val_categorical_accuracy: 0.5913 - lr: 2.0000e-04\n",
      "Epoch 9/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 1.0388 - categorical_accuracy: 0.5941\n",
      "Epoch 9: val_categorical_accuracy improved from 0.59135 to 0.59426, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 1.0388 - categorical_accuracy: 0.5942 - val_loss: 1.0375 - val_categorical_accuracy: 0.5943 - lr: 2.0000e-04\n",
      "Epoch 10/300\n",
      "1873/1879 [============================>.] - ETA: 0s - loss: 1.0316 - categorical_accuracy: 0.5974\n",
      "Epoch 10: val_categorical_accuracy improved from 0.59426 to 0.59759, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 18s 9ms/step - loss: 1.0316 - categorical_accuracy: 0.5973 - val_loss: 1.0296 - val_categorical_accuracy: 0.5976 - lr: 2.0000e-04\n",
      "Epoch 11/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 1.0261 - categorical_accuracy: 0.5997\n",
      "Epoch 11: val_categorical_accuracy improved from 0.59759 to 0.60076, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 18s 10ms/step - loss: 1.0260 - categorical_accuracy: 0.5997 - val_loss: 1.0253 - val_categorical_accuracy: 0.6008 - lr: 2.0000e-04\n",
      "Epoch 12/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 1.0212 - categorical_accuracy: 0.6019\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.60076\n",
      "1879/1879 [==============================] - 18s 10ms/step - loss: 1.0212 - categorical_accuracy: 0.6019 - val_loss: 1.0213 - val_categorical_accuracy: 0.6006 - lr: 2.0000e-04\n",
      "Epoch 13/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 1.0171 - categorical_accuracy: 0.6039\n",
      "Epoch 13: val_categorical_accuracy improved from 0.60076 to 0.60279, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 18s 10ms/step - loss: 1.0171 - categorical_accuracy: 0.6039 - val_loss: 1.0184 - val_categorical_accuracy: 0.6028 - lr: 2.0000e-04\n",
      "Epoch 14/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 1.0137 - categorical_accuracy: 0.6055\n",
      "Epoch 14: val_categorical_accuracy improved from 0.60279 to 0.60423, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 1.0137 - categorical_accuracy: 0.6055 - val_loss: 1.0133 - val_categorical_accuracy: 0.6042 - lr: 2.0000e-04\n",
      "Epoch 15/300\n",
      "1877/1879 [============================>.] - ETA: 0s - loss: 1.0105 - categorical_accuracy: 0.6072\n",
      "Epoch 15: val_categorical_accuracy improved from 0.60423 to 0.60576, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 1.0105 - categorical_accuracy: 0.6072 - val_loss: 1.0130 - val_categorical_accuracy: 0.6058 - lr: 2.0000e-04\n",
      "Epoch 16/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 1.0075 - categorical_accuracy: 0.6091\n",
      "Epoch 16: val_categorical_accuracy improved from 0.60576 to 0.60891, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 1.0074 - categorical_accuracy: 0.6092 - val_loss: 1.0077 - val_categorical_accuracy: 0.6089 - lr: 2.0000e-04\n",
      "Epoch 17/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 1.0046 - categorical_accuracy: 0.6102\n",
      "Epoch 17: val_categorical_accuracy improved from 0.60891 to 0.61033, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 1.0045 - categorical_accuracy: 0.6103 - val_loss: 1.0050 - val_categorical_accuracy: 0.6103 - lr: 2.0000e-04\n",
      "Epoch 18/300\n",
      "1878/1879 [============================>.] - ETA: 0s - loss: 1.0015 - categorical_accuracy: 0.6116\n",
      "Epoch 18: val_categorical_accuracy did not improve from 0.61033\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 1.0015 - categorical_accuracy: 0.6116 - val_loss: 1.0034 - val_categorical_accuracy: 0.6098 - lr: 2.0000e-04\n",
      "Epoch 19/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 0.9990 - categorical_accuracy: 0.6136\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.61033\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 0.9990 - categorical_accuracy: 0.6136 - val_loss: 1.0001 - val_categorical_accuracy: 0.6103 - lr: 2.0000e-04\n",
      "Epoch 20/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 0.9968 - categorical_accuracy: 0.6145\n",
      "Epoch 20: val_categorical_accuracy improved from 0.61033 to 0.61299, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9968 - categorical_accuracy: 0.6145 - val_loss: 0.9988 - val_categorical_accuracy: 0.6130 - lr: 2.0000e-04\n",
      "Epoch 21/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 0.9941 - categorical_accuracy: 0.6161\n",
      "Epoch 21: val_categorical_accuracy improved from 0.61299 to 0.61429, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 15s 8ms/step - loss: 0.9942 - categorical_accuracy: 0.6160 - val_loss: 0.9951 - val_categorical_accuracy: 0.6143 - lr: 2.0000e-04\n",
      "Epoch 22/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9918 - categorical_accuracy: 0.6174\n",
      "Epoch 22: val_categorical_accuracy did not improve from 0.61429\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9918 - categorical_accuracy: 0.6174 - val_loss: 0.9950 - val_categorical_accuracy: 0.6128 - lr: 2.0000e-04\n",
      "Epoch 23/300\n",
      "1878/1879 [============================>.] - ETA: 0s - loss: 0.9896 - categorical_accuracy: 0.6188\n",
      "Epoch 23: val_categorical_accuracy improved from 0.61429 to 0.61791, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9896 - categorical_accuracy: 0.6188 - val_loss: 0.9940 - val_categorical_accuracy: 0.6179 - lr: 2.0000e-04\n",
      "Epoch 24/300\n",
      "1876/1879 [============================>.] - ETA: 0s - loss: 0.9876 - categorical_accuracy: 0.6196\n",
      "Epoch 24: val_categorical_accuracy improved from 0.61791 to 0.61990, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 0.9877 - categorical_accuracy: 0.6196 - val_loss: 0.9887 - val_categorical_accuracy: 0.6199 - lr: 2.0000e-04\n",
      "Epoch 25/300\n",
      "1876/1879 [============================>.] - ETA: 0s - loss: 0.9858 - categorical_accuracy: 0.6205\n",
      "Epoch 25: val_categorical_accuracy did not improve from 0.61990\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9858 - categorical_accuracy: 0.6205 - val_loss: 0.9877 - val_categorical_accuracy: 0.6191 - lr: 2.0000e-04\n",
      "Epoch 26/300\n",
      "1873/1879 [============================>.] - ETA: 0s - loss: 0.9840 - categorical_accuracy: 0.6217\n",
      "Epoch 26: val_categorical_accuracy improved from 0.61990 to 0.62139, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9840 - categorical_accuracy: 0.6217 - val_loss: 0.9863 - val_categorical_accuracy: 0.6214 - lr: 2.0000e-04\n",
      "Epoch 27/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9822 - categorical_accuracy: 0.6226\n",
      "Epoch 27: val_categorical_accuracy improved from 0.62139 to 0.62165, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9821 - categorical_accuracy: 0.6226 - val_loss: 0.9836 - val_categorical_accuracy: 0.6217 - lr: 2.0000e-04\n",
      "Epoch 28/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 0.9807 - categorical_accuracy: 0.6233\n",
      "Epoch 28: val_categorical_accuracy improved from 0.62165 to 0.62323, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9808 - categorical_accuracy: 0.6233 - val_loss: 0.9813 - val_categorical_accuracy: 0.6232 - lr: 2.0000e-04\n",
      "Epoch 29/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 0.9791 - categorical_accuracy: 0.6246\n",
      "Epoch 29: val_categorical_accuracy did not improve from 0.62323\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 0.9791 - categorical_accuracy: 0.6246 - val_loss: 0.9822 - val_categorical_accuracy: 0.6200 - lr: 2.0000e-04\n",
      "Epoch 30/300\n",
      "1873/1879 [============================>.] - ETA: 0s - loss: 0.9772 - categorical_accuracy: 0.6257\n",
      "Epoch 30: val_categorical_accuracy improved from 0.62323 to 0.62386, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9773 - categorical_accuracy: 0.6257 - val_loss: 0.9805 - val_categorical_accuracy: 0.6239 - lr: 2.0000e-04\n",
      "Epoch 31/300\n",
      "1878/1879 [============================>.] - ETA: 0s - loss: 0.9760 - categorical_accuracy: 0.6262\n",
      "Epoch 31: val_categorical_accuracy improved from 0.62386 to 0.62627, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9760 - categorical_accuracy: 0.6262 - val_loss: 0.9780 - val_categorical_accuracy: 0.6263 - lr: 2.0000e-04\n",
      "Epoch 32/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 0.9751 - categorical_accuracy: 0.6263\n",
      "Epoch 32: val_categorical_accuracy improved from 0.62627 to 0.62723, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9751 - categorical_accuracy: 0.6263 - val_loss: 0.9753 - val_categorical_accuracy: 0.6272 - lr: 2.0000e-04\n",
      "Epoch 33/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9733 - categorical_accuracy: 0.6275\n",
      "Epoch 33: val_categorical_accuracy did not improve from 0.62723\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 0.9733 - categorical_accuracy: 0.6275 - val_loss: 0.9752 - val_categorical_accuracy: 0.6248 - lr: 2.0000e-04\n",
      "Epoch 34/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9723 - categorical_accuracy: 0.6275\n",
      "Epoch 34: val_categorical_accuracy improved from 0.62723 to 0.62749, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 0.9724 - categorical_accuracy: 0.6275 - val_loss: 0.9726 - val_categorical_accuracy: 0.6275 - lr: 2.0000e-04\n",
      "Epoch 35/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 0.9713 - categorical_accuracy: 0.6284\n",
      "Epoch 35: val_categorical_accuracy did not improve from 0.62749\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9713 - categorical_accuracy: 0.6284 - val_loss: 0.9778 - val_categorical_accuracy: 0.6216 - lr: 2.0000e-04\n",
      "Epoch 36/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9699 - categorical_accuracy: 0.6290\n",
      "Epoch 36: val_categorical_accuracy did not improve from 0.62749\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9699 - categorical_accuracy: 0.6291 - val_loss: 0.9739 - val_categorical_accuracy: 0.6272 - lr: 2.0000e-04\n",
      "Epoch 37/300\n",
      "1878/1879 [============================>.] - ETA: 0s - loss: 0.9691 - categorical_accuracy: 0.6297\n",
      "Epoch 37: val_categorical_accuracy improved from 0.62749 to 0.62798, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9691 - categorical_accuracy: 0.6297 - val_loss: 0.9721 - val_categorical_accuracy: 0.6280 - lr: 2.0000e-04\n",
      "Epoch 38/300\n",
      "1872/1879 [============================>.] - ETA: 0s - loss: 0.9677 - categorical_accuracy: 0.6302\n",
      "Epoch 38: val_categorical_accuracy did not improve from 0.62798\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9678 - categorical_accuracy: 0.6301 - val_loss: 0.9713 - val_categorical_accuracy: 0.6278 - lr: 2.0000e-04\n",
      "Epoch 39/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9671 - categorical_accuracy: 0.6309\n",
      "Epoch 39: val_categorical_accuracy improved from 0.62798 to 0.62886, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9671 - categorical_accuracy: 0.6308 - val_loss: 0.9715 - val_categorical_accuracy: 0.6289 - lr: 2.0000e-04\n",
      "Epoch 40/300\n",
      "1877/1879 [============================>.] - ETA: 0s - loss: 0.9659 - categorical_accuracy: 0.6313\n",
      "Epoch 40: val_categorical_accuracy improved from 0.62886 to 0.62953, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9659 - categorical_accuracy: 0.6313 - val_loss: 0.9692 - val_categorical_accuracy: 0.6295 - lr: 2.0000e-04\n",
      "Epoch 41/300\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 0.9653 - categorical_accuracy: 0.6316\n",
      "Epoch 41: val_categorical_accuracy improved from 0.62953 to 0.63277, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9653 - categorical_accuracy: 0.6316 - val_loss: 0.9667 - val_categorical_accuracy: 0.6328 - lr: 2.0000e-04\n",
      "Epoch 42/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9640 - categorical_accuracy: 0.6323\n",
      "Epoch 42: val_categorical_accuracy did not improve from 0.63277\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9639 - categorical_accuracy: 0.6323 - val_loss: 0.9674 - val_categorical_accuracy: 0.6303 - lr: 2.0000e-04\n",
      "Epoch 43/300\n",
      "1875/1879 [============================>.] - ETA: 0s - loss: 0.9630 - categorical_accuracy: 0.6330\n",
      "Epoch 43: val_categorical_accuracy did not improve from 0.63277\n",
      "1879/1879 [==============================] - 17s 9ms/step - loss: 0.9630 - categorical_accuracy: 0.6330 - val_loss: 0.9663 - val_categorical_accuracy: 0.6312 - lr: 2.0000e-04\n",
      "Epoch 44/300\n",
      "1873/1879 [============================>.] - ETA: 0s - loss: 0.9629 - categorical_accuracy: 0.6326\n",
      "Epoch 44: val_categorical_accuracy did not improve from 0.63277\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9627 - categorical_accuracy: 0.6327 - val_loss: 0.9698 - val_categorical_accuracy: 0.6285 - lr: 2.0000e-04\n",
      "Epoch 45/300\n",
      "1874/1879 [============================>.] - ETA: 0s - loss: 0.9618 - categorical_accuracy: 0.6336\n",
      "Epoch 45: val_categorical_accuracy did not improve from 0.63277\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9618 - categorical_accuracy: 0.6336 - val_loss: 0.9664 - val_categorical_accuracy: 0.6289 - lr: 2.0000e-04\n",
      "Epoch 46/300\n",
      "1876/1879 [============================>.] - ETA: 0s - loss: 0.9606 - categorical_accuracy: 0.6341\n",
      "Epoch 46: val_categorical_accuracy improved from 0.63277 to 0.63441, saving model to model_QGCN_nconst16_nbits8_Seed175_Kfold_20231023-054956/model_QGCN_nconst_16_nbits_8_kfold_0.h5\n",
      "1879/1879 [==============================] - 16s 9ms/step - loss: 0.9607 - categorical_accuracy: 0.6341 - val_loss: 0.9626 - val_categorical_accuracy: 0.6344 - lr: 2.0000e-04\n",
      "Epoch 47/300\n",
      "1876/1879 [============================>.] - ETA: 0s - loss: 0.9598 - categorical_accuracy: 0.6346\n",
      "Epoch 47: val_categorical_accuracy did not improve from 0.63441\n",
      "1879/1879 [==============================] - 16s 8ms/step - loss: 0.9598 - categorical_accuracy: 0.6346 - val_loss: 0.9616 - val_categorical_accuracy: 0.6343 - lr: 2.0000e-04\n",
      "Epoch 48/300\n",
      "1667/1879 [=========================>....] - ETA: 1s - loss: 0.9583 - categorical_accuracy: 0.6348"
     ]
    }
   ],
   "source": [
    "import util.data\n",
    "import util.plots\n",
    "import util.util\n",
    "from util.terminal_colors import tcols\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "nmax = nconstit\n",
    "accuracy_keras = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop of training folds\n",
    "with tf.device('CPU: 0'):   # disable M1 metal GPU \n",
    "  for i in range (kfolds):\n",
    "  \n",
    "    # Load weights initialization values before any training ( reset weights ) \n",
    "    model.load_weights('model.h5')\n",
    "    \n",
    "    print(\"train kfold num:\", i)\n",
    "    val_kfold = i \n",
    "\n",
    "    #test_kfold = args.TK\n",
    "    train_kfolds = [kfold for kfold in range(kfolds) if kfold != val_kfold]\n",
    "\n",
    "    \n",
    "    fpath = f'./data_kfold/jets_{nmax}constituents_ptetaphi_nonorm'\n",
    "    fnames_train = [f'jet_images_c{nmax}_minpt2.0_ptetaphi_nonorm_{train_kfold}'\n",
    "                     for train_kfold in train_kfolds]\n",
    "    fname_val = f'jet_images_c{nmax}_minpt2.0_ptetaphi_nonorm_{val_kfold}'\n",
    "\n",
    "    data = util.data.Data.load_kfolds(fpath, fnames_train, fname_val)\n",
    "    print (data.train_data.shape)\n",
    "\n",
    "\n",
    "    X_train = data.train_data\n",
    "    X_val = data.test_data\n",
    "    X_test = data.test_data\n",
    "\n",
    "    Y_train = data.train_target\n",
    "    Y_val = data.test_target\n",
    "    Y_test = data.test_target    \n",
    "  \n",
    "\n",
    "    # Nomalization\n",
    "    interquantile_range_32 = [120, 0.27, 0.27]\n",
    "    interquantile_range_16 = [166, 0.24, 0.24]\n",
    "    interquantile_range_8  = [219, 0.20, 0.20]\n",
    "    \n",
    "    if nmax == 8:\n",
    "        X_train = X_train / interquantile_range_8\n",
    "        X_val   = X_val   / interquantile_range_8\n",
    "        X_test  = X_test  / interquantile_range_8\n",
    "    elif nmax == 16:\n",
    "        X_train = X_train / interquantile_range_16\n",
    "        X_val   = X_val   / interquantile_range_16\n",
    "        X_test  = X_test  / interquantile_range_16\n",
    "    elif nmax == 32:\n",
    "        X_train = X_train / interquantile_range_32\n",
    "        X_val   = X_val   / interquantile_range_32\n",
    "        X_test  = X_test  / interquantile_range_32\n",
    "\n",
    "    # Flatten data for MLP input\n",
    "    if (arch=='QMLP'):        \n",
    "        X_train = X_train.reshape(-1, NINPUT)\n",
    "        X_val   = X_val.reshape(-1, NINPUT)\n",
    "        X_test  = X_test.reshape(-1, NINPUT)\n",
    "    \n",
    "    \n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "    print( f\"number of G jets for training/validation: {np.sum(np.argmax(Y_train, axis=1) == 0)}/{np.sum(np.argmax(Y_val, axis=1) == 0)}\")\n",
    "    print( f\"number of Q jets for training/validation: {np.sum(np.argmax(Y_train, axis=1) == 1)}/{np.sum(np.argmax(Y_val, axis=1) == 1)}\")\n",
    "    print( f\"number of W jets for training/validation: {np.sum(np.argmax(Y_train, axis=1) == 2)}/{np.sum(np.argmax(Y_val, axis=1) == 2)}\")\n",
    "    print( f\"number of Z jets for training/validation: {np.sum(np.argmax(Y_train, axis=1) == 3)}/{np.sum(np.argmax(Y_val, axis=1) == 3)}\")\n",
    "    print( f\"number of T jets for training/validation: {np.sum(np.argmax(Y_train, axis=1) == 4)}/{np.sum(np.argmax(Y_val, axis=1) == 4)}\")\n",
    "\n",
    "    \n",
    "    print(\"number of G jets for testing: %i\" % np.sum(np.argmax(Y_test, axis=1) == 0))\n",
    "    print(\"number of Q jets for testing: %i\" % np.sum(np.argmax(Y_test, axis=1) == 1))\n",
    "    print(\"number of W jets for testing: %i\" % np.sum(np.argmax(Y_test, axis=1) == 2))\n",
    "    print(\"number of Z jets for testing: %i\" % np.sum(np.argmax(Y_test, axis=1) == 3))\n",
    "    print(\"number of T jets for testing: %i\" % np.sum(np.argmax(Y_test, axis=1) == 4))\n",
    "    \n",
    "#######################################################################################\n",
    "\n",
    "    mname = \"model_{}_nconst_{}_nbits_{}_kfold_{}\".format(arch,nmax,nbits,val_kfold)\n",
    "\n",
    "    if(i==0):\n",
    "        outputdir = \"model_{}_nconst{}_nbits{}_Seed{}_Kfold_{}\".format(\n",
    "            arch,\n",
    "            nconstit,\n",
    "            nbits,\n",
    "            seed,\n",
    "            time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        )\n",
    "        print(\"output dir: \", outputdir)\n",
    "        os.mkdir(outputdir)\n",
    "\n",
    "        # early stopping callback\n",
    "        es = EarlyStopping(monitor=\"val_categorical_accuracy\", patience=patience)\n",
    "        # Learning rate scheduler\n",
    "        ls = ReduceLROnPlateau(monitor=\"val_categorical_accuracy\", factor=0.2, patience=patience)\n",
    "\n",
    "    # Train classifier\n",
    "    chkp = ModelCheckpoint(outputdir+\"/\"+mname+\".h5\",\n",
    "            monitor=\"val_categorical_accuracy\",\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode=\"auto\",\n",
    "            save_freq=\"epoch\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs=nepochs,\n",
    "        batch_size=batch,  # small batch\n",
    "        verbose=1,\n",
    "        callbacks=[es, ls, chkp],\n",
    "        #validation_split=0.2,\n",
    "        validation_data=(X_val, Y_val))\n",
    "\n",
    "    # Retrieve the best model\n",
    "    model = tf.keras.models.load_model(\n",
    "        \"{}/{}.h5\".format(outputdir, mname),\n",
    "        custom_objects={\n",
    "            \"QDense\": QDense,\n",
    "            \"QActivation\": QActivation,\n",
    "            \"QConv1D\": QConv1D,\n",
    "            \"QConv2D\": QConv2D,\n",
    "            \"quantized_bits\": quantized_bits,\n",
    "#            \"NodeEdgeProjection\": NodeEdgeProjection,\n",
    "#            \"PruneLowMagnitude\": pruning_wrapper.PruneLowMagnitude,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Get predictions for the best model\n",
    "    y_keras = model.predict(X_test)\n",
    "    \n",
    "    # Store best model predictions\n",
    "    accuracy_keras.append(float(\n",
    "        accuracy_score(np.argmax(Y_test, axis=1), np.argmax(y_keras, axis=1))\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c3e32-6edc-4c67-b4b7-f8a043d83c4c",
   "metadata": {},
   "source": [
    "## Saves accuracy and errors for paper plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a15a8a-fa81-440d-a6b0-4494796d5b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#        kfold_metrics = {\n",
    "#            \"fprs\": [],\n",
    "#            \"aucs\": [],\n",
    "#            \"fats\": [],\n",
    "#            \"accs\": [],\n",
    "#            \"loss\": []\n",
    "#        }\n",
    "\n",
    "#plots_dir = outputdir\n",
    "#\n",
    "#roc_metrics = util.plots.roc_curves(plots_dir, y_keras, data.test_target)\n",
    "#util.plots.dnn_output(plots_dir, y_keras)\n",
    "\n",
    "accuracy_average = np.mean(np.array(accuracy_keras))\n",
    "accuracy_errs = np.std(np.array(accuracy_keras))\n",
    "\n",
    "\n",
    "accs = np.zeros(3)\n",
    "accs[0] = accuracy_average\n",
    "accs[2] = accuracy_errs\n",
    "\n",
    "np.savetxt(\"{}/acc.txt\".format(outputdir), accs, fmt=\"%.6f\")\n",
    "print(\"Keras:\\n\", accuracy_keras)\n",
    "\n",
    "print(\"output dir: \", outputdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9e41c-d3c3-4758-811d-0d67e186b48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7833c9-cc89-4cff-a563-822075bbdd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52061e4a-562d-44f3-8a4b-9f85c7be6ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be774cea-7ff6-456f-8fa1-a8410b6d806d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0810dc-b01c-4a98-9699-24c3a017bcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d668a31-8a5f-4cf9-85f5-09f0667f344a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
